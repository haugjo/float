<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>float.feature_selection API documentation</title>
<meta name="description" content="The float.feature_selector module includes online feature selection methods." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>float.feature_selection</code></h1>
</header>
<section id="section-intro">
<p>The float.feature_selector module includes online feature selection methods.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
The float.feature_selector module includes online feature selection methods.
&#34;&#34;&#34;

from .feature_selector import FeatureSelector
from .cancel_out import CancelOut
from .efs import EFS
from .fires import FIRES
from .fsds import FSDS
from .ofs import OFS

__all__ = [&#39;FeatureSelector&#39;, &#39;CancelOut&#39;, &#39;EFS&#39;, &#39;FIRES&#39;, &#39;FSDS&#39;, &#39;OFS&#39;]</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="float.feature_selection.cancel_out" href="cancel_out.html">float.feature_selection.cancel_out</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="float.feature_selection.efs" href="efs.html">float.feature_selection.efs</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="float.feature_selection.feature_selector" href="feature_selector.html">float.feature_selection.feature_selector</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="float.feature_selection.fires" href="fires.html">float.feature_selection.fires</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="float.feature_selection.fsds" href="fsds.html">float.feature_selection.fsds</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="float.feature_selection.ofs" href="ofs.html">float.feature_selection.ofs</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="float.feature_selection.CancelOut"><code class="flex name class">
<span>class <span class="ident">CancelOut</span></span>
<span>(</span><span>X, *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>A CancelOut Layer.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X</code></strong></dt>
<dd>an input data (vector, matrix, tensor)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CancelOut(nn.Module):
    &#34;&#34;&#34;
    A CancelOut Layer.
    &#34;&#34;&#34;
    def __init__(self, X, *args, **kwargs):
        &#34;&#34;&#34;

        Args:
            X: an input data (vector, matrix, tensor)
        &#34;&#34;&#34;
        super(CancelOut, self).__init__()
        self.weights = nn.Parameter(torch.zeros(X, requires_grad=True) + 4)

    def forward(self, X):
        return X * torch.sigmoid(self.weights.float())</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="float.feature_selection.CancelOut.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="float.feature_selection.CancelOut.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="float.feature_selection.CancelOut.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, X) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, X):
    return X * torch.sigmoid(self.weights.float())</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="float.feature_selection.EFS"><code class="flex name class">
<span>class <span class="ident">EFS</span></span>
<span>(</span><span>n_total_features, n_selected_features, evaluation_metrics=None, u=None, v=None, theta=1, M=1, alpha=1.5, beta=0.5)</span>
</code></dt>
<dd>
<div class="desc"><p>Extremal Feature Selection.</p>
<p>Based on a paper by Carvalho et al. 2005. This Feature Selection algorithm is based on the weights of a
Modified Balanced Winnow classifier (as introduced in the paper).</p>
<p>Initializes the EFS feature selector.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>n_total_features</code></strong> :&ensp;<code>int</code></dt>
<dd>total number of features</dd>
<dt><strong><code>n_selected_features</code></strong> :&ensp;<code>int</code></dt>
<dd>number of selected features</dd>
<dt>evaluation_metrics (dict of str: function | dict of str: (function, dict)): {metric_name: metric_function} OR {metric_name: (metric_function, {param_name1: param_val1, &hellip;})} a dictionary of metrics to be used</dt>
<dt><strong><code>u</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>initial positive model with weights set to 2</dd>
<dt><strong><code>v</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>initial negative model with weights</dd>
<dt><strong><code>theta</code></strong> :&ensp;<code><a title="float" href="../index.html">float</a></code></dt>
<dd>threshold parameter</dd>
<dt><strong><code>M</code></strong> :&ensp;<code><a title="float" href="../index.html">float</a></code></dt>
<dd>margin parameter</dd>
<dt><strong><code>alpha</code></strong> :&ensp;<code><a title="float" href="../index.html">float</a></code></dt>
<dd>promotion parameter</dd>
<dt><strong><code>beta</code></strong> :&ensp;<code><a title="float" href="../index.html">float</a></code></dt>
<dd>demotion parameter</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class EFS(FeatureSelector):
    &#34;&#34;&#34;
    Extremal Feature Selection.

    Based on a paper by Carvalho et al. 2005. This Feature Selection algorithm is based on the weights of a
    Modified Balanced Winnow classifier (as introduced in the paper).
    &#34;&#34;&#34;
    def __init__(self, n_total_features, n_selected_features, evaluation_metrics=None, u=None, v=None, theta=1, M=1, alpha=1.5, beta=0.5):
        &#34;&#34;&#34;
        Initializes the EFS feature selector.

        Args:
            n_total_features (int): total number of features
            n_selected_features (int): number of selected features
            evaluation_metrics (dict of str: function | dict of str: (function, dict)): {metric_name: metric_function} OR {metric_name: (metric_function, {param_name1: param_val1, ...})} a dictionary of metrics to be used
            u (np.ndarray): initial positive model with weights set to 2
            v (np.ndarray): initial negative model with weights
            theta (float): threshold parameter
            M (float): margin parameter
            alpha (float): promotion parameter
            beta (float): demotion parameter
        &#34;&#34;&#34;
        super().__init__(n_total_features, n_selected_features, evaluation_metrics, supports_multi_class=False,
                         supports_streaming_features=False)

        self.u = np.ones(n_total_features) * 2 if u is None else u
        self.v = np.ones(n_total_features) if v is None else v

        self.theta = theta
        self.M = M
        self.alpha = alpha
        self.beta = beta

    def weight_features(self, X, y):
        &#34;&#34;&#34;
        Given a batch of observations and corresponding labels, computes feature weights.

        Args:
            X (np.ndarray): samples of current batch
            y (np.ndarray): labels of current batch
        &#34;&#34;&#34;
        # iterate over all elements in batch
        for x_b, y_b in zip(X, y):

            # Convert label to -1 and 1
            y_b = -1 if y_b == 0 else 1

            # Note, the original algorithm here adds a &#34;bias&#34; feature that is always 1

            # Normalize x_b
            x_b = MinMaxScaler().fit_transform(x_b.reshape(-1, 1)).flatten()

            # Calculate score
            score = np.dot(x_b, self.u) - np.dot(x_b, self.v) - self.theta

            # If prediction was mistaken
            if score * y_b &lt;= self.M:
                # Update models for all features j
                for j, _ in enumerate(self.u):
                    if y_b &gt; 0:
                        self.u[j] = self.u[j] * self.alpha * (1 + x_b[j])
                        self.v[j] = self.v[j] * self.beta * (1 - x_b[j])
                    else:
                        self.u[j] = self.u[j] * self.beta * (1 - x_b[j])
                        self.v[j] = self.v[j] * self.alpha * (1 + x_b[j])

        # Compute importance score of features
        self.raw_weight_vector = abs(self.u - self.v)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="float.feature_selection.feature_selector.FeatureSelector" href="feature_selector.html#float.feature_selection.feature_selector.FeatureSelector">FeatureSelector</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="float.feature_selection.feature_selector.FeatureSelector" href="feature_selector.html#float.feature_selection.feature_selector.FeatureSelector">FeatureSelector</a></b></code>:
<ul class="hlist">
<li><code><a title="float.feature_selection.feature_selector.FeatureSelector.evaluate" href="feature_selector.html#float.feature_selection.feature_selector.FeatureSelector.evaluate">evaluate</a></code></li>
<li><code><a title="float.feature_selection.feature_selector.FeatureSelector.get_nogueira_stability" href="feature_selector.html#float.feature_selection.feature_selector.FeatureSelector.get_nogueira_stability">get_nogueira_stability</a></code></li>
<li><code><a title="float.feature_selection.feature_selector.FeatureSelector.select_features" href="feature_selector.html#float.feature_selection.feature_selector.FeatureSelector.select_features">select_features</a></code></li>
<li><code><a title="float.feature_selection.feature_selector.FeatureSelector.weight_features" href="feature_selector.html#float.feature_selection.feature_selector.FeatureSelector.weight_features">weight_features</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="float.feature_selection.FIRES"><code class="flex name class">
<span>class <span class="ident">FIRES</span></span>
<span>(</span><span>n_total_features, n_selected_features, classes, evaluation_metrics=None, mu_init=0, sigma_init=1, penalty_s=0.01, penalty_r=0.01, epochs=1, lr_mu=0.01, lr_sigma=0.01, scale_weights=True, model='probit')</span>
</code></dt>
<dd>
<div class="desc"><p>FIRES: Fast, Interpretable and Robust Evaluation and Selection of features</p>
<p>cite: Haug et al. 2020. Leveraging Model Inherent Variable Importance for Stable Online Feature Selection.
In Proceedings of the 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’20),
August 23–27, 2020, Virtual Event, CA, USA.</p>
<p>Initializes the FIRES feature selector.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>n_total_features</code></strong> :&ensp;<code>int</code></dt>
<dd>total number of features</dd>
<dt><strong><code>n_selected_features</code></strong> :&ensp;<code>int</code></dt>
<dd>number of selected features</dd>
<dt><strong><code>classes</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>unique target values (class labels)</dd>
<dt>evaluation_metrics (dict of str: function | dict of str: (function, dict)): {metric_name: metric_function} OR {metric_name: (metric_function, {param_name1: param_val1, &hellip;})} a dictionary of metrics to be used</dt>
<dt>mu_init (int | np.ndarray): initial importance parameter</dt>
<dt>sigma_init (int | np.ndarray): initial uncertainty parameter</dt>
<dt><strong><code>penalty_s</code></strong> :&ensp;<code><a title="float" href="../index.html">float</a></code></dt>
<dd>penalty factor for the uncertainty (corresponds to gamma_s in the paper)</dd>
<dt><strong><code>penalty_r</code></strong> :&ensp;<code><a title="float" href="../index.html">float</a></code></dt>
<dd>penalty factor for the regularization (corresponds to gamma_r in the paper)</dd>
<dt><strong><code>epochs</code></strong> :&ensp;<code>int</code></dt>
<dd>number of epochs that we use each batch of observations to update the parameters</dd>
<dt><strong><code>lr_mu</code></strong> :&ensp;<code><a title="float" href="../index.html">float</a></code></dt>
<dd>learning rate for the gradient update of the importance</dd>
<dt><strong><code>lr_sigma</code></strong> :&ensp;<code><a title="float" href="../index.html">float</a></code></dt>
<dd>learning rate for the gradient update of the uncertainty</dd>
<dt><strong><code>scale_weights</code></strong> :&ensp;<code>bool</code></dt>
<dd>if True, scale feature weights into the range [0,1]</dd>
<dt><strong><code>model</code></strong> :&ensp;<code>str</code></dt>
<dd>name of the base model to compute the likelihood (default is 'probit')</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FIRES(FeatureSelector):
    &#34;&#34;&#34;
    FIRES: Fast, Interpretable and Robust Evaluation and Selection of features

    cite: Haug et al. 2020. Leveraging Model Inherent Variable Importance for Stable Online Feature Selection.
    In Proceedings of the 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’20),
    August 23–27, 2020, Virtual Event, CA, USA.
    &#34;&#34;&#34;
    def __init__(self, n_total_features, n_selected_features, classes, evaluation_metrics=None, mu_init=0, sigma_init=1, penalty_s=0.01, penalty_r=0.01, epochs=1,
                 lr_mu=0.01, lr_sigma=0.01, scale_weights=True, model=&#39;probit&#39;):
        &#34;&#34;&#34;
        Initializes the FIRES feature selector.

        Args:
            n_total_features (int): total number of features
            n_selected_features (int): number of selected features
            classes (np.ndarray): unique target values (class labels)
            evaluation_metrics (dict of str: function | dict of str: (function, dict)): {metric_name: metric_function} OR {metric_name: (metric_function, {param_name1: param_val1, ...})} a dictionary of metrics to be used
            mu_init (int | np.ndarray): initial importance parameter
            sigma_init (int | np.ndarray): initial uncertainty parameter
            penalty_s (float): penalty factor for the uncertainty (corresponds to gamma_s in the paper)
            penalty_r (float): penalty factor for the regularization (corresponds to gamma_r in the paper)
            epochs (int): number of epochs that we use each batch of observations to update the parameters
            lr_mu (float): learning rate for the gradient update of the importance
            lr_sigma (float): learning rate for the gradient update of the uncertainty
            scale_weights (bool): if True, scale feature weights into the range [0,1]
            model (str): name of the base model to compute the likelihood (default is &#39;probit&#39;)
        &#34;&#34;&#34;
        super().__init__(n_total_features, n_selected_features, evaluation_metrics, supports_multi_class=False,
                         supports_streaming_features=False)
        self.n_total_ftr = n_total_features
        self.classes = classes
        self.mu = np.ones(n_total_features) * mu_init
        self.sigma = np.ones(n_total_features) * sigma_init
        self.penalty_s = penalty_s
        self.penalty_r = penalty_r
        self.epochs = epochs
        self.lr_mu = lr_mu
        self.lr_sigma = lr_sigma
        self.scale_weights = scale_weights
        self.model = model

        # Additional model-specific parameters
        self.model_param = {}

        # Probit model
        if self.model == &#39;probit&#39; and tuple(classes) != (-1, 1):
            if len(np.unique(classes)) == 2:
                self.model_param[&#39;probit&#39;] = True  # Indicates that we need to encode the target variable into {-1,1}
                warn(&#39;FIRES WARNING: The target variable will be encoded as: {} = -1, {} = 1&#39;.format(
                    self.classes[0], self.classes[1]))
            else:
                raise ValueError(&#39;The target variable y must be binary.&#39;)

    def weight_features(self, X, y):
        &#34;&#34;&#34;
        Given a batch of observations and corresponding labels, computes feature weights.

        Args:
            X (np.ndarray): samples of current batch
            y (np.ndarray): labels of current batch

        Returns:
            np.ndarray: feature weights
        &#34;&#34;&#34;
        # Update estimates of mu and sigma given the predictive model
        if self.model == &#39;probit&#39;:
            self.__probit(X, y)
        # ### ADD YOUR OWN MODEL HERE ##################################################
        # elif self.model == &#39;your_model&#39;:
        #    self.__yourModel(x, y)
        ################################################################################
        else:
            raise NotImplementedError(&#39;The given model name does not exist&#39;)

        # Limit sigma to range [0, inf]
        if sum(n &lt; 0 for n in self.sigma) &gt; 0:
            self.sigma[self.sigma &lt; 0] = 0
            warn(&#39;Sigma has automatically been rescaled to [0, inf], because it contained negative values.&#39;)

        # Compute feature weights
        self.raw_weight_vector = self.__compute_weights()

    def __probit(self, X, y):
        &#34;&#34;&#34;
        Updates the distribution parameters mu and sigma by optimizing them in terms of the (log) likelihood.
        Here we assume a Bernoulli distributed target variable. We use a Probit model as our base model.
        This corresponds to the FIRES-GLM model in the paper.

        Args:
            X (np.ndarray): batch of observations (numeric values only, consider normalizing data for better results)
            y (np.ndarray): batch of labels: type binary, i.e. {-1,1} (bool, int or str will be encoded accordingly)
        &#34;&#34;&#34;

        for epoch in range(self.epochs):
            # Shuffle the observations
            random_idx = np.random.permutation(len(y))
            X = X[random_idx]
            y = y[random_idx]

            # Encode target as {-1,1}
            if &#39;probit&#39; in self.model_param:
                y[y == self.classes[0]] = -1
                y[y == self.classes[1]] = 1

            # Iterative update of mu and sigma
            try:
                # Helper functions
                dot_mu_x = np.dot(X, self.mu)
                rho = np.sqrt(1 + np.dot(X ** 2, self.sigma ** 2))

                # Gradients
                nabla_mu = norm.pdf(y / rho * dot_mu_x) * (y / rho * X.T)
                nabla_sigma = norm.pdf(y / rho * dot_mu_x) * (
                        - y / (2 * rho ** 3) * 2 * (X ** 2 * self.sigma).T * dot_mu_x)

                # Marginal Likelihood
                marginal = norm.cdf(y / rho * dot_mu_x)

                # Update parameters
                self.mu += self.lr_mu * np.mean(nabla_mu / marginal, axis=1)
                self.sigma += self.lr_sigma * np.mean(nabla_sigma / marginal, axis=1)
            except TypeError as e:
                raise TypeError(&#39;All features must be a numeric data type.&#39;) from e

    def __compute_weights(self):
        &#34;&#34;&#34;
        Computes optimal weights according to the objective function proposed in the paper.
        We compute feature weights in a trade-off between feature importance and uncertainty.
        Thereby, we aim to maximize both the discriminative power and the stability/robustness of feature weights.

        Returns:
            np.ndarray: feature weights
        &#34;&#34;&#34;
        # Compute optimal weights
        weights = (self.mu ** 2 - self.penalty_s * self.sigma ** 2) / (2 * self.penalty_r)

        if self.scale_weights:  # Scale weights to [0,1]
            weights = MinMaxScaler().fit_transform(weights.reshape(-1, 1)).flatten()

        return weights</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="float.feature_selection.feature_selector.FeatureSelector" href="feature_selector.html#float.feature_selection.feature_selector.FeatureSelector">FeatureSelector</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="float.feature_selection.FIRES.weight_features"><code class="name flex">
<span>def <span class="ident">weight_features</span></span>(<span>self, X, y)</span>
</code></dt>
<dd>
<div class="desc"><p>Given a batch of observations and corresponding labels, computes feature weights.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>samples of current batch</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>labels of current batch</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>feature weights</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def weight_features(self, X, y):
    &#34;&#34;&#34;
    Given a batch of observations and corresponding labels, computes feature weights.

    Args:
        X (np.ndarray): samples of current batch
        y (np.ndarray): labels of current batch

    Returns:
        np.ndarray: feature weights
    &#34;&#34;&#34;
    # Update estimates of mu and sigma given the predictive model
    if self.model == &#39;probit&#39;:
        self.__probit(X, y)
    # ### ADD YOUR OWN MODEL HERE ##################################################
    # elif self.model == &#39;your_model&#39;:
    #    self.__yourModel(x, y)
    ################################################################################
    else:
        raise NotImplementedError(&#39;The given model name does not exist&#39;)

    # Limit sigma to range [0, inf]
    if sum(n &lt; 0 for n in self.sigma) &gt; 0:
        self.sigma[self.sigma &lt; 0] = 0
        warn(&#39;Sigma has automatically been rescaled to [0, inf], because it contained negative values.&#39;)

    # Compute feature weights
    self.raw_weight_vector = self.__compute_weights()</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="float.feature_selection.feature_selector.FeatureSelector" href="feature_selector.html#float.feature_selection.feature_selector.FeatureSelector">FeatureSelector</a></b></code>:
<ul class="hlist">
<li><code><a title="float.feature_selection.feature_selector.FeatureSelector.evaluate" href="feature_selector.html#float.feature_selection.feature_selector.FeatureSelector.evaluate">evaluate</a></code></li>
<li><code><a title="float.feature_selection.feature_selector.FeatureSelector.get_nogueira_stability" href="feature_selector.html#float.feature_selection.feature_selector.FeatureSelector.get_nogueira_stability">get_nogueira_stability</a></code></li>
<li><code><a title="float.feature_selection.feature_selector.FeatureSelector.select_features" href="feature_selector.html#float.feature_selection.feature_selector.FeatureSelector.select_features">select_features</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="float.feature_selection.FSDS"><code class="flex name class">
<span>class <span class="ident">FSDS</span></span>
<span>(</span><span>n_total_features, n_selected_features, evaluation_metrics=None, l=0, m=None, B=None, k=2, nogueira_window_size=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Feature Selection on Data Streams.</p>
<p>Based on a paper by Huang et al. (2015). Feature Selection for unsupervised Learning.
This code is copied from the Python implementation of the authors with minor reductions and adaptations.</p>
<p>Initializes the FSDS feature selector.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>n_total_features</code></strong> :&ensp;<code>int</code></dt>
<dd>total number of features</dd>
<dt><strong><code>n_selected_features</code></strong> :&ensp;<code>int</code></dt>
<dd>number of selected features</dd>
<dt>evaluation_metrics (dict of str: function | dict of str: (function, dict)): {metric_name: metric_function} OR {metric_name: (metric_function, {param_name1: param_val1, &hellip;})} a dictionary of metrics to be used</dt>
<dt><strong><code>l</code></strong> :&ensp;<code>int</code></dt>
<dd>size of the matrix sketch with l &lt;&lt; m</dd>
<dt><strong><code>m</code></strong> :&ensp;<code>int</code></dt>
<dd>size of the feature space</dd>
<dt>B (list/np.ndarray): matrix sketch</dt>
<dt><strong><code>k</code></strong> :&ensp;<code>int</code></dt>
<dd>number of singular vectors with k &lt;= ell</dd>
<dt><strong><code>nogueira_window_size</code></strong> :&ensp;<code>int</code></dt>
<dd>window size for the Nogueira stability measure</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FSDS(FeatureSelector):
    &#34;&#34;&#34;
    Feature Selection on Data Streams.

    Based on a paper by Huang et al. (2015). Feature Selection for unsupervised Learning.
    This code is copied from the Python implementation of the authors with minor reductions and adaptations.
    &#34;&#34;&#34;
    def __init__(self, n_total_features, n_selected_features, evaluation_metrics=None, l=0, m=None, B=None, k=2, nogueira_window_size=None):
        &#34;&#34;&#34;
        Initializes the FSDS feature selector.

        Args:
            n_total_features (int): total number of features
            n_selected_features (int): number of selected features
            evaluation_metrics (dict of str: function | dict of str: (function, dict)): {metric_name: metric_function} OR {metric_name: (metric_function, {param_name1: param_val1, ...})} a dictionary of metrics to be used
            l (int): size of the matrix sketch with l &lt;&lt; m
            m (int): size of the feature space
            B (list/np.ndarray): matrix sketch
            k (int): number of singular vectors with k &lt;= ell
            nogueira_window_size (int): window size for the Nogueira stability measure
        &#34;&#34;&#34;
        super().__init__(n_total_features, n_selected_features, evaluation_metrics, supports_multi_class=False,
                         supports_streaming_features=False)

        self.m = n_total_features if m is None else m
        self.B = [] if B is None else B
        self.l = l
        self.k = k

    def weight_features(self, X, y):
        &#34;&#34;&#34;
        Given a batch of observations and corresponding labels, computes feature weights.

        Args:
            X (np.ndarray): samples of current batch
            y (np.ndarray): labels of current batch
        &#34;&#34;&#34;
        Yt = X.T  # algorithm assumes rows to represent features

        if self.l &lt; 1:
            self.l = int(np.sqrt(self.m))

        if len(self.B) == 0:
            # for Y0, we need to first create an initial sketched matrix
            self.B = Yt[:, :self.l]
            C = np.hstack((self.B, Yt[:, self.l:]))
            n = Yt.shape[1] - self.l
        else:
            # combine current sketched matrix with input at time t
            # C: m-by-(n+ell) matrix
            C = np.hstack((self.B, Yt))
            n = Yt.shape[1]

        U, s, V = ln.svd(C, full_matrices=False)
        U = U[:, :self.l]
        s = s[:self.l]
        V = V[:, :self.l]

        # shrink step in Frequent Directions algorithm
        # (shrink singular values based on the squared smallest singular value)
        delta = s[-1] ** 2
        s = np.sqrt(s ** 2 - delta)

        # -- Extension of original code --
        # replace nan values with 0 to prevent division by zero error for small batch numbers
        s = np.nan_to_num(s)

        # update sketched matrix B
        # (focus on column singular vectors)
        self.B = np.dot(U, np.diag(s))

        # According to Section 5.1, for all experiments,
        # the authors set alpha = 2^3 * sigma_k based on the pre-experiment
        alpha = (2 ** 3) * s[self.k - 1]

        # solve the ridge regression by using the top-k singular values
        # X: m-by-k matrix (k &lt;= ell)
        D = np.diag(s[:self.k] / (s[:self.k] ** 2 + alpha))

        # -- Extension of original code --
        # replace nan values with 0 to prevent division by zero error for small batch numbers
        D = np.nan_to_num(D)

        X = np.dot(U[:, :self.k], D)

        self.raw_weight_vector = np.amax(abs(X), axis=1)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="float.feature_selection.feature_selector.FeatureSelector" href="feature_selector.html#float.feature_selection.feature_selector.FeatureSelector">FeatureSelector</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="float.feature_selection.feature_selector.FeatureSelector" href="feature_selector.html#float.feature_selection.feature_selector.FeatureSelector">FeatureSelector</a></b></code>:
<ul class="hlist">
<li><code><a title="float.feature_selection.feature_selector.FeatureSelector.evaluate" href="feature_selector.html#float.feature_selection.feature_selector.FeatureSelector.evaluate">evaluate</a></code></li>
<li><code><a title="float.feature_selection.feature_selector.FeatureSelector.get_nogueira_stability" href="feature_selector.html#float.feature_selection.feature_selector.FeatureSelector.get_nogueira_stability">get_nogueira_stability</a></code></li>
<li><code><a title="float.feature_selection.feature_selector.FeatureSelector.select_features" href="feature_selector.html#float.feature_selection.feature_selector.FeatureSelector.select_features">select_features</a></code></li>
<li><code><a title="float.feature_selection.feature_selector.FeatureSelector.weight_features" href="feature_selector.html#float.feature_selection.feature_selector.FeatureSelector.weight_features">weight_features</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="float.feature_selection.FeatureSelector"><code class="flex name class">
<span>class <span class="ident">FeatureSelector</span></span>
<span>(</span><span>n_total_features, n_selected_features, evaluation_metrics, supports_multi_class, supports_streaming_features, streaming_features=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Abstract base class for online feature selection methods.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>n_total_features</code></strong> :&ensp;<code>int</code></dt>
<dd>total number of features</dd>
<dt><strong><code>n_selected_features</code></strong> :&ensp;<code>int</code></dt>
<dd>number of selected features</dd>
<dt>evaluation (dict of str: list[float]): a dictionary of metric names and their corresponding metric values as lists</dt>
<dt><strong><code>supports_multi_class</code></strong> :&ensp;<code>bool</code></dt>
<dd>True if model support multi-class classification, False otherwise</dd>
<dt><strong><code>supports_streaming_features</code></strong> :&ensp;<code>bool</code></dt>
<dd>True if model supports streaming features, False otherwise</dd>
<dt><strong><code>raw_weight_vector</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>current weights (as produced by feature selection model)</dd>
<dt><strong><code>weights</code></strong> :&ensp;<code>list</code></dt>
<dd>absolute weights in all time steps</dd>
<dt><strong><code>selection</code></strong> :&ensp;<code>list</code></dt>
<dd>indices of selected features in all time steps</dd>
<dt><strong><code>comp_times</code></strong> :&ensp;<code>list</code></dt>
<dd>computation time in all time steps</dd>
</dl>
<p>Receives parameters of feature selection model.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>n_total_features</code></strong> :&ensp;<code>int</code></dt>
<dd>total number of features</dd>
<dt><strong><code>n_selected_features</code></strong> :&ensp;<code>int</code></dt>
<dd>number of selected features</dd>
<dt>evaluation_metrics (dict of str: function | dict of str: (function, dict)): {metric_name: metric_function} OR {metric_name: (metric_function, {param_name1: param_val1, &hellip;})} a dictionary of metrics to be used</dt>
<dt><strong><code>supports_multi_class</code></strong> :&ensp;<code>bool</code></dt>
<dd>True if model support multi-class classification, False otherwise</dd>
<dt><strong><code>supports_streaming_features</code></strong> :&ensp;<code>bool</code></dt>
<dd>True if model supports streaming features, False otherwise</dd>
<dt><strong><code>streaming_features</code></strong> :&ensp;<code>dict</code></dt>
<dd>(time, feature index) tuples to simulate streaming features</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FeatureSelector(metaclass=ABCMeta):
    &#34;&#34;&#34;
    Abstract base class for online feature selection methods.

    Attributes:
        n_total_features (int): total number of features
        n_selected_features (int): number of selected features
        evaluation (dict of str: list[float]): a dictionary of metric names and their corresponding metric values as lists
        supports_multi_class (bool): True if model support multi-class classification, False otherwise
        supports_streaming_features (bool): True if model supports streaming features, False otherwise
        raw_weight_vector (np.ndarray): current weights (as produced by feature selection model)
        weights (list): absolute weights in all time steps
        selection (list): indices of selected features in all time steps
        comp_times (list): computation time in all time steps
    &#34;&#34;&#34;

    def __init__(self, n_total_features, n_selected_features, evaluation_metrics, supports_multi_class,
                 supports_streaming_features, streaming_features=None):
        &#34;&#34;&#34;
        Receives parameters of feature selection model.

        Args:
            n_total_features (int): total number of features
            n_selected_features (int): number of selected features
            evaluation_metrics (dict of str: function | dict of str: (function, dict)): {metric_name: metric_function} OR {metric_name: (metric_function, {param_name1: param_val1, ...})} a dictionary of metrics to be used
            supports_multi_class (bool): True if model support multi-class classification, False otherwise
            supports_streaming_features (bool): True if model supports streaming features, False otherwise
            streaming_features (dict): (time, feature index) tuples to simulate streaming features
        &#34;&#34;&#34;
        self.n_total_features = n_total_features
        self.n_selected_features = n_selected_features
        self.evaluation_metrics = evaluation_metrics if evaluation_metrics else {&#39;Nogueira Stability Measure&#39;: (FeatureSelector.get_nogueira_stability, {&#39;n_total_features&#39;: self.n_total_features, &#39;nogueira_window_size&#39;: 10})}
        self.evaluation = {key: [] for key in self.evaluation_metrics.keys()}

        self.supports_multi_class = supports_multi_class
        self.supports_streaming_features = supports_streaming_features
        self.streaming_features = streaming_features if streaming_features else dict()

        self.raw_weight_vector = np.zeros(self.n_total_features)
        self.weights = []
        self.selection = []
        self.comp_times = []
        self.selected_features = []
        self._auto_scale = False

    @abstractmethod
    def weight_features(self, X, y):
        &#34;&#34;&#34;
        Given a batch of observations and corresponding labels, computes feature weights.

        Args:
            X (np.ndarray): samples of current batch
            y (np.ndarray): labels of current batch
        &#34;&#34;&#34;
        raise NotImplementedError

    def select_features(self, X, time_step):
        &#34;&#34;&#34;
        Selects features with highest absolute weights.

        Args:
            X (np.ndarray): the data samples
            time_step (int): the current time step

        Returns:
            np.ndarray: the data samples with the non-selected features set to a reference value
        &#34;&#34;&#34;
        if self.supports_streaming_features:
            if time_step == 0 and time_step not in self.streaming_features:
                self.selected_features = np.arange(self.n_total_features)
                warnings.warn(
                    &#39;Simulate streaming features: No active features provided at t=0. All features are used instead.&#39;)
            elif time_step in self.streaming_features:
                self.selected_features = self.streaming_features[time_step]
        else:
            if np.any(self.raw_weight_vector &lt; 0):
                abs_weights = abs(self.raw_weight_vector)
                if not self._auto_scale:
                    warnings.warn(&#39;Weight vector contains negative weights. Absolute weights will be used for feature&#39;
                                  &#39; selection.&#39;)
                    self._auto_scale = True
            else:
                abs_weights = self.raw_weight_vector

            sorted_indices = np.argsort(abs_weights)[::-1]
            self.selected_features = sorted_indices[:self.n_selected_features]
            self.weights.append(abs_weights.tolist())
            self.selection.append(self.selected_features.tolist())

        X_new = np.full(X.shape, self._get_reference_value())
        X_new[:, self.selected_features] = X[:, self.selected_features]
        return X_new

    def evaluate(self, time_step):
        &#34;&#34;&#34;
        Evaluates the feature selector at one time step.
        &#34;&#34;&#34;
        for metric_name in self.evaluation:
            if isinstance(self.evaluation_metrics[metric_name], tuple):
                metric_func = self.evaluation_metrics[metric_name][0]
                metric_params = self.evaluation_metrics[metric_name][1]
            else:
                metric_func = self.evaluation_metrics[metric_name]
                metric_params = {}
            try:
                metric_val = metric_func(self.selection, **metric_params)
            except TypeError:
                if time_step == 0:
                    traceback.print_exc()
                continue

            self.evaluation[metric_name].append(metric_val)

    @staticmethod
    def get_nogueira_stability(selection, n_total_features, nogueira_window_size=10):
        &#34;&#34;&#34;
        Returns the Nogueira measure for feature selection stability.

        Returns:
            float: the stability measure
        &#34;&#34;&#34;
        Z = np.zeros([min(len(selection), nogueira_window_size), n_total_features])
        for row, col in enumerate(selection[-nogueira_window_size:]):
            Z[row, col] = 1

        try:
            M, d = Z.shape
            hatPF = np.mean(Z, axis=0)
            kbar = np.sum(hatPF)
            denom = (kbar / d) * (1 - kbar / d)
            stability_measure = 1 - (M / (M - 1)) * np.mean(np.multiply(hatPF, 1 - hatPF)) / denom
        except ZeroDivisionError:
            stability_measure = 0  # metric requires at least 2 measurements and thus runs an error at t=1

        return stability_measure

    def _get_reference_value(self):
        &#34;&#34;&#34;
        Returns the reference value to be used for the non-selected features.

        Returns:
            float: the reference value
        &#34;&#34;&#34;
        return 0</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="float.feature_selection.cancel_out.CancelOutFeatureSelector" href="cancel_out.html#float.feature_selection.cancel_out.CancelOutFeatureSelector">CancelOutFeatureSelector</a></li>
<li><a title="float.feature_selection.efs.EFS" href="efs.html#float.feature_selection.efs.EFS">EFS</a></li>
<li><a title="float.feature_selection.fires.FIRES" href="fires.html#float.feature_selection.fires.FIRES">FIRES</a></li>
<li><a title="float.feature_selection.fsds.FSDS" href="fsds.html#float.feature_selection.fsds.FSDS">FSDS</a></li>
<li><a title="float.feature_selection.ofs.OFS" href="ofs.html#float.feature_selection.ofs.OFS">OFS</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="float.feature_selection.FeatureSelector.get_nogueira_stability"><code class="name flex">
<span>def <span class="ident">get_nogueira_stability</span></span>(<span>selection, n_total_features, nogueira_window_size=10)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the Nogueira measure for feature selection stability.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="float" href="../index.html">float</a></code></dt>
<dd>the stability measure</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def get_nogueira_stability(selection, n_total_features, nogueira_window_size=10):
    &#34;&#34;&#34;
    Returns the Nogueira measure for feature selection stability.

    Returns:
        float: the stability measure
    &#34;&#34;&#34;
    Z = np.zeros([min(len(selection), nogueira_window_size), n_total_features])
    for row, col in enumerate(selection[-nogueira_window_size:]):
        Z[row, col] = 1

    try:
        M, d = Z.shape
        hatPF = np.mean(Z, axis=0)
        kbar = np.sum(hatPF)
        denom = (kbar / d) * (1 - kbar / d)
        stability_measure = 1 - (M / (M - 1)) * np.mean(np.multiply(hatPF, 1 - hatPF)) / denom
    except ZeroDivisionError:
        stability_measure = 0  # metric requires at least 2 measurements and thus runs an error at t=1

    return stability_measure</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="float.feature_selection.FeatureSelector.evaluate"><code class="name flex">
<span>def <span class="ident">evaluate</span></span>(<span>self, time_step)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluates the feature selector at one time step.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluate(self, time_step):
    &#34;&#34;&#34;
    Evaluates the feature selector at one time step.
    &#34;&#34;&#34;
    for metric_name in self.evaluation:
        if isinstance(self.evaluation_metrics[metric_name], tuple):
            metric_func = self.evaluation_metrics[metric_name][0]
            metric_params = self.evaluation_metrics[metric_name][1]
        else:
            metric_func = self.evaluation_metrics[metric_name]
            metric_params = {}
        try:
            metric_val = metric_func(self.selection, **metric_params)
        except TypeError:
            if time_step == 0:
                traceback.print_exc()
            continue

        self.evaluation[metric_name].append(metric_val)</code></pre>
</details>
</dd>
<dt id="float.feature_selection.FeatureSelector.select_features"><code class="name flex">
<span>def <span class="ident">select_features</span></span>(<span>self, X, time_step)</span>
</code></dt>
<dd>
<div class="desc"><p>Selects features with highest absolute weights.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>the data samples</dd>
<dt><strong><code>time_step</code></strong> :&ensp;<code>int</code></dt>
<dd>the current time step</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>the data samples with the non-selected features set to a reference value</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def select_features(self, X, time_step):
    &#34;&#34;&#34;
    Selects features with highest absolute weights.

    Args:
        X (np.ndarray): the data samples
        time_step (int): the current time step

    Returns:
        np.ndarray: the data samples with the non-selected features set to a reference value
    &#34;&#34;&#34;
    if self.supports_streaming_features:
        if time_step == 0 and time_step not in self.streaming_features:
            self.selected_features = np.arange(self.n_total_features)
            warnings.warn(
                &#39;Simulate streaming features: No active features provided at t=0. All features are used instead.&#39;)
        elif time_step in self.streaming_features:
            self.selected_features = self.streaming_features[time_step]
    else:
        if np.any(self.raw_weight_vector &lt; 0):
            abs_weights = abs(self.raw_weight_vector)
            if not self._auto_scale:
                warnings.warn(&#39;Weight vector contains negative weights. Absolute weights will be used for feature&#39;
                              &#39; selection.&#39;)
                self._auto_scale = True
        else:
            abs_weights = self.raw_weight_vector

        sorted_indices = np.argsort(abs_weights)[::-1]
        self.selected_features = sorted_indices[:self.n_selected_features]
        self.weights.append(abs_weights.tolist())
        self.selection.append(self.selected_features.tolist())

    X_new = np.full(X.shape, self._get_reference_value())
    X_new[:, self.selected_features] = X[:, self.selected_features]
    return X_new</code></pre>
</details>
</dd>
<dt id="float.feature_selection.FeatureSelector.weight_features"><code class="name flex">
<span>def <span class="ident">weight_features</span></span>(<span>self, X, y)</span>
</code></dt>
<dd>
<div class="desc"><p>Given a batch of observations and corresponding labels, computes feature weights.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>samples of current batch</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>labels of current batch</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def weight_features(self, X, y):
    &#34;&#34;&#34;
    Given a batch of observations and corresponding labels, computes feature weights.

    Args:
        X (np.ndarray): samples of current batch
        y (np.ndarray): labels of current batch
    &#34;&#34;&#34;
    raise NotImplementedError</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="float.feature_selection.OFS"><code class="flex name class">
<span>class <span class="ident">OFS</span></span>
<span>(</span><span>n_total_features, n_selected_features, evaluation_metrics=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Online Feature Selection.</p>
<p>Based on a paper by Wang et al. 2014. Feature Selection for binary classification.
This code is an adaptation of the official Matlab implementation.</p>
<p>Initializes the OFS feature selector.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>n_total_features</code></strong> :&ensp;<code>int</code></dt>
<dd>total number of features</dd>
<dt><strong><code>n_selected_features</code></strong> :&ensp;<code>int</code></dt>
<dd>number of selected features</dd>
</dl>
<p>evaluation_metrics (dict of str: function | dict of str: (function, dict)): {metric_name: metric_function} OR {metric_name: (metric_function, {param_name1: param_val1, &hellip;})} a dictionary of metrics to be used</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class OFS(FeatureSelector):
    &#34;&#34;&#34;
    Online Feature Selection.

    Based on a paper by Wang et al. 2014. Feature Selection for binary classification.
    This code is an adaptation of the official Matlab implementation.
    &#34;&#34;&#34;
    def __init__(self, n_total_features, n_selected_features, evaluation_metrics=None):
        &#34;&#34;&#34;
        Initializes the OFS feature selector.

        Args:
            n_total_features (int): total number of features
            n_selected_features (int): number of selected features
            evaluation_metrics (dict of str: function | dict of str: (function, dict)): {metric_name: metric_function} OR {metric_name: (metric_function, {param_name1: param_val1, ...})} a dictionary of metrics to be used
        &#34;&#34;&#34;
        super().__init__(n_total_features, n_selected_features, evaluation_metrics, supports_multi_class=False,
                         supports_streaming_features=False)

    def weight_features(self, X, y):
        &#34;&#34;&#34;
        Given a batch of observations and corresponding labels, computes feature weights.

        Args:
            X (np.ndarray): samples of current batch
            y (np.ndarray): labels of current batch
        &#34;&#34;&#34;
        eta = 0.2
        lamb = 0.01

        for x_b, y_b in zip(X, y):  # perform feature selection for each instance in batch
            # Convert label to -1 and 1
            y_b = -1 if y_b == 0 else 1

            f = np.dot(self.raw_weight_vector, x_b)  # prediction

            if y_b * f &lt;= 1:  # update classifier w
                self.raw_weight_vector = self.raw_weight_vector + eta * y_b * x_b
                self.raw_weight_vector = self.raw_weight_vector * min(1, 1 / (math.sqrt(lamb) * np.linalg.norm(self.raw_weight_vector)))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="float.feature_selection.feature_selector.FeatureSelector" href="feature_selector.html#float.feature_selection.feature_selector.FeatureSelector">FeatureSelector</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="float.feature_selection.feature_selector.FeatureSelector" href="feature_selector.html#float.feature_selection.feature_selector.FeatureSelector">FeatureSelector</a></b></code>:
<ul class="hlist">
<li><code><a title="float.feature_selection.feature_selector.FeatureSelector.evaluate" href="feature_selector.html#float.feature_selection.feature_selector.FeatureSelector.evaluate">evaluate</a></code></li>
<li><code><a title="float.feature_selection.feature_selector.FeatureSelector.get_nogueira_stability" href="feature_selector.html#float.feature_selection.feature_selector.FeatureSelector.get_nogueira_stability">get_nogueira_stability</a></code></li>
<li><code><a title="float.feature_selection.feature_selector.FeatureSelector.select_features" href="feature_selector.html#float.feature_selection.feature_selector.FeatureSelector.select_features">select_features</a></code></li>
<li><code><a title="float.feature_selection.feature_selector.FeatureSelector.weight_features" href="feature_selector.html#float.feature_selection.feature_selector.FeatureSelector.weight_features">weight_features</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="float" href="../index.html">float</a></code></li>
</ul>
</li>
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="float.feature_selection.cancel_out" href="cancel_out.html">float.feature_selection.cancel_out</a></code></li>
<li><code><a title="float.feature_selection.efs" href="efs.html">float.feature_selection.efs</a></code></li>
<li><code><a title="float.feature_selection.feature_selector" href="feature_selector.html">float.feature_selection.feature_selector</a></code></li>
<li><code><a title="float.feature_selection.fires" href="fires.html">float.feature_selection.fires</a></code></li>
<li><code><a title="float.feature_selection.fsds" href="fsds.html">float.feature_selection.fsds</a></code></li>
<li><code><a title="float.feature_selection.ofs" href="ofs.html">float.feature_selection.ofs</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="float.feature_selection.CancelOut" href="#float.feature_selection.CancelOut">CancelOut</a></code></h4>
<ul class="">
<li><code><a title="float.feature_selection.CancelOut.dump_patches" href="#float.feature_selection.CancelOut.dump_patches">dump_patches</a></code></li>
<li><code><a title="float.feature_selection.CancelOut.forward" href="#float.feature_selection.CancelOut.forward">forward</a></code></li>
<li><code><a title="float.feature_selection.CancelOut.training" href="#float.feature_selection.CancelOut.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="float.feature_selection.EFS" href="#float.feature_selection.EFS">EFS</a></code></h4>
</li>
<li>
<h4><code><a title="float.feature_selection.FIRES" href="#float.feature_selection.FIRES">FIRES</a></code></h4>
<ul class="">
<li><code><a title="float.feature_selection.FIRES.weight_features" href="#float.feature_selection.FIRES.weight_features">weight_features</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="float.feature_selection.FSDS" href="#float.feature_selection.FSDS">FSDS</a></code></h4>
</li>
<li>
<h4><code><a title="float.feature_selection.FeatureSelector" href="#float.feature_selection.FeatureSelector">FeatureSelector</a></code></h4>
<ul class="">
<li><code><a title="float.feature_selection.FeatureSelector.evaluate" href="#float.feature_selection.FeatureSelector.evaluate">evaluate</a></code></li>
<li><code><a title="float.feature_selection.FeatureSelector.get_nogueira_stability" href="#float.feature_selection.FeatureSelector.get_nogueira_stability">get_nogueira_stability</a></code></li>
<li><code><a title="float.feature_selection.FeatureSelector.select_features" href="#float.feature_selection.FeatureSelector.select_features">select_features</a></code></li>
<li><code><a title="float.feature_selection.FeatureSelector.weight_features" href="#float.feature_selection.FeatureSelector.weight_features">weight_features</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="float.feature_selection.OFS" href="#float.feature_selection.OFS">OFS</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>